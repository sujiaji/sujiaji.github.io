<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://sujiaji.github.io/homepage-test/feed.xml" rel="self" type="application/atom+xml"/><link href="https://sujiaji.github.io/homepage-test/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-04-10T03:11:36+00:00</updated><id>https://sujiaji.github.io/homepage-test/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html"></title><link href="https://sujiaji.github.io/homepage-test/blog/2024/2024-04-10-mf/" rel="alternate" type="text/html" title=""/><published>2024-04-10T03:11:36+00:00</published><updated>2024-04-10T03:11:36+00:00</updated><id>https://sujiaji.github.io/homepage-test/blog/2024/2024-04-10-mf</id><content type="html" xml:base="https://sujiaji.github.io/homepage-test/blog/2024/2024-04-10-mf/"><![CDATA[<h3 id="overview">Overview</h3> <div class="justify-text"> In recent years, there has been a growing interest in non-Euclidean statistical analysis, particularly in the pursuit of recovering a low-dimensional structure, referred to as a manifold, that underlies high-dimensional data. The successful recovery of this manifold is contingent on certain noise concentration conditions. Previous methods tackle this challenge by approximating the manifold based on tangent space estimations at each data sample. While these methods offer theoretical convergence guarantees, they assume either noise-free data or noise with bounded characteristics. In practical scenarios where unbounded noise is common, the tangent space estimations at noisy samples become inherently imprecise, potentially introducing inaccuracies when fitting the manifold. <br/><br/> In response to this challenge, our research project introduces a novel approach. Instead of estimating tangent spaces at the original data samples, we directly estimate these spaces at projected points on the underlying manifold. This strategic shift aims to mitigate errors caused by unbounded noise, resulting in more accurate manifold fitting. <br/><br/> Our research, encompassing our 2019 paper (yx) and subsequent work in 2023 (ysl), centers on non-Euclidean statistical analysis, specifically the recovery of low-dimensional manifolds from high-dimensional data. Unlike existing methods relying on tangent space estimations at data samples, we directly estimate these spaces at manifold-projected points, enhancing accuracy and addressing unbounded noise. Our initial paper (yx) introduced a practical algorithm for manifold fitting, and our 2023 work (ysl) refines the algorithm and establishes superior error bounds. These contributions significantly advance non-Euclidean statistical analysis. </div> <h3 id="model-setting">Model Setting</h3> <p>Let $\mathcal{M}$ be a $d$-dimensional smooth latent manifold embedded in the ambient space $\mathbb{R}^D$. In this problem, we focus on a random vector $Y \in \mathbb{R}^D$ that can be expressed as</p> \[Y = X + \xi,\] <p>where $X \in \mathbb{R}^D$ is an unobserved random vector following a distribution $\omega$ supported on the latent manifold $\mathcal{M}$, and $\xi \sim \phi_\sigma$ represents the ambient-space observation noise, independent of $X$, with a standard deviation $\sigma$. The distribution of $Y$ can be viewed as the convolution of $\omega$ and $\phi_\sigma$, whose density at point $y$ can be expressed as</p> \[\nu(y) = \int_\mathcal{M} \phi_\sigma(y-x)\omega(x)d x.\] <p>Assume $\mathcal{Y} = {y_i}_{i=1}^N \subset \mathbb{R}^D$ is the collection of observed data points, also in the form of</p> \[y_i = x_i + \xi_i, \quad \text{ for } i = 1,\cdots,N,\] <p>with $(y_i, x_i,\xi_i)$ being $N$ independent and identical realizations of $(Y,X,\xi)$. Based on $\mathcal{Y}$, we construct an estimator $\widehat{\mathcal{M}}$ for $\mathcal{M}$ and provide theoretical justification for it under the following main assumptions:</p> <ul> <li> <p>The latent manifold $\mathcal{M}$ is a compact and twice-differentiable $d$-dimensional sub-manifold, embedded in the ambient space $\mathbb{R}^D$. Its volume with respect to the $d$-dimensional Hausdorff measure is upper bounded by $V$, and its reach<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup> is lower bounded by a fixed constant $\tau$.</p> </li> <li> <p>The distribution $\omega$ is a smooth distribution, with respect to the $d$-dimensional Hausdorff measure, on $\mathcal{M}$.</p> </li> <li> <p>The noise distribution $\phi_\sigma$ is a Gaussian distribution supported on $\mathbb{R}^D$ with density function</p> \[\phi_\sigma (\xi)= (\frac{1}{2\pi \sigma^2})^{\frac{D}{2}}\exp{(-\frac{\|\xi\|_2^2}{2\sigma^2})}.\] </li> <li> <p>The intrinsic dimension $d$ and noise standard deviation $\sigma$ are known.</p> </li> </ul> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1" role="doc-endnote"> <p>A non-negative quantity measuring the curvature of a manifold.Â <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author></entry></feed>