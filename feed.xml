<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://sujiaji.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://sujiaji.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-05-29T04:10:45+00:00</updated><id>https://sujiaji.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">How to fit a manifold?</title><link href="https://sujiaji.github.io/blog/2024/mf-2/" rel="alternate" type="text/html" title="How to fit a manifold?"/><published>2024-10-25T04:00:00+00:00</published><updated>2024-10-25T04:00:00+00:00</updated><id>https://sujiaji.github.io/blog/2024/mf-2</id><content type="html" xml:base="https://sujiaji.github.io/blog/2024/mf-2/"><![CDATA[<p>This is a quick introduction of manifold fitting. A <a href="/assets/pdf/Quick_Notes_on_Manifold_Fitting.pdf">PDF version</a> is also available. More details can be found in:</p> <ul> <li>Fefferman, C., Ivanov, S., Kurylev, Y., Lassas, M., &amp; Narayanan, H. (2018, July). <a href="https://proceedings.mlr.press/v75/fefferman18a.html">Fitting a putative manifold to noisy data</a>. <em>In Conference On Learning Theory</em> (pp. 688-720). PMLR.</li> <li>Yao, Z., &amp; Xia, Y. (2019). <a href="https://arxiv.org/abs/1909.10228">Manifold fitting under unbounded noise</a>. <em>arXiv preprint arXiv:1909.10228</em>.</li> <li>Yao, Z., Su, J., Li, B., &amp; Yau, S. T. (2023). <a href="https://arxiv.org/abs/2304.07680">Manifold fitting</a>. <em>arXiv preprint arXiv:2304.07680</em>.</li> </ul> <h3 id="method">Method</h3> <p>Let \(z\) be the point of interest, which is close to \(\mathcal{M}\), and \(z^* = \arg\min_{z^\prime\in\mathcal{M}} d(z^\prime,z)\) be the projection of \(z\) on \(\mathcal{M}\). Intuitively, the estimation of manifold can be viewed as ‘‘pushing’’ \(z\) to \(z^*\).</p> <div style="text-align: center;"> <img src="/assets/img/mf-pics/push.png" alt="Push" style="max-width: 60%; height: auto;"/> </div> <p>This pushing process involves two key components: direction and distance. The direction should be perpendicular to \(T_{z*}\mathcal{M}\), which can be deduced from the local ‘‘covariance’’ structure, while the distance \(d(z,\mathcal{M})\) might be estimated using the local average. The following subsections will introduce some intuitive concepts related to this process. For more details, please refer to the papers mentioned previously.</p> <h4 id="estimate-direction-from-local-covariance">Estimate Direction from Local ‘‘Covariance’’</h4> <p>For each point \(y_i\in\mathcal{Y}_N\), let \(y_i^*\) be its projection on \(\mathcal{M}\). Assume the normal space of \(\mathcal{M}\) at \(y_i^*\) has orthonormal basis \(\{u_1,\dots,u_{D-d}\}\), we use</p> \[\Pi_{y_i^*}^\perp = \bigl(u_1,\dots,u_{D-d}\bigl) \bigl(u_1,\dots,u_{D-d}\bigl)^\top = \sum_{k=1}^{D-d} u_k u_k^\top\] <p>to represent the projection matrix onto this space. This projection matrix can be estimated from the local variation centered at \(y_i\), and the estimator of \(\Pi_{y_i^*}^\perp\) is denoted as \(\widehat{\Pi}_{y_i}^\perp\).</p> <div style="text-align: center;"> <img src="/assets/img/mf-pics/local-cov.png" alt="Local Covariance" style="max-width: 60%; height: auto;"/> </div> <p>Let \(\mathcal{B}_D(y_i,r)\) be the \(D\)-dimensional Euclidean ball centered at \(y_i\) with radius \(r\). If \(r\) is ‘‘large’’ enough, such that \(\|y_i - y_i^*\| \leq \|\xi_i\|\leq c_0r\), the area of \(\mathcal{B}_D(y_i,r)\cap \mathcal{M}\) roughly has radius \(c_1 r\), and the variation of \(\mathcal{M}\) along the normal direction is less than \(c_2 r^2\) due to the reach. Then, since the distribution of \(X\) is smooth, the variation of \(Y-y_i\) along direction:</p> <ul> <li>\(\leftrightarrow\): is roughly in the order of \((c_1 r)^2 + \sigma^2\);</li> <li>\(\updownarrow\): is roughly bounded above by the order of \((c_2 r^2)^2 + \sigma^2\).</li> </ul> <p>Thus, we can define</p> \[\widehat\Sigma_{r,i} = \frac{\sum_{j=1}^N (y_j-y_i)(y_j-y_i)^\top \mathbb{I}(\|y_j-y_i\|\leq r)}{\sum_{j=1}^n \mathbb{I}(\|y_j-y_i\|\leq r)}.\] <p>Then perform SVD on \(\widehat\Sigma_{r,i}\) to obtain \(\{\lambda_1&lt;\cdots&lt;\lambda_D\}\) and \(\{v_1,\dots,v_D\}\), and estimate \(\Pi_{y_i^*}^\perp\) with \(\widehat{\Pi}_{y_i}^\perp = \bigl(v_1,\dots,v_{D-d}\bigl) \bigl(v_1,\dots,v_{D-d}\bigl)^\top = \sum_{k=1}^{D-d} v_k v_k^\top,\) whose estimation error can be bounded.</p> <h4 id="smoothing-system">Smoothing System</h4> <p>To make the overall estimation smooth enough, the weight function for \(y_i\) with respect to \(z\) is defined as</p> \[\widetilde{\alpha}_i(z)=\left(1 - \frac{\|z - y_i\|^2}{r^{\prime 2}}\right)^{\beta} \mathbb{I}(\|z-y_i\|\leq r^\prime), \quad \alpha_i(z) = \frac{\widetilde{\alpha}_i(z)}{\sum_{i=1}^n\widetilde{\alpha}_i(z)},\] <p>where \(\beta\geq 2\) is a parameter corresponding to the smoothness. Then, for \(z\), a smooth reference point can be given by \(\widehat{\mu}_z = \sum_{i=1}^N \alpha_i(z) y_i\), and a smooth projection matrix is calculated as</p> \[\Psi_z = \mathbb{P}_{D-d}\left(\sum_{i=1}^N\alpha_i(z)\widehat\Pi_{y_i}^\perp\right),\] <p>where \(\mathbb{P}_{k} (A)\) stands for the projection of matrix \(A\) onto the span space corresponding to its largest \(k\) eigenvalues.</p> <h4 id="the-manifold-estimator">The Manifold Estimator</h4> <p>The vector from \(z^*\) to \(z\) can be estimated with the bias vector</p> \[\widehat{b}(z) = \sum_{i=1}^n \alpha_i(z)\Psi_z (z - y_i) = \Psi_z (z - \widehat{\mu}_z),\] <p>which can be shown</p> <ul> <li>\(\|\widehat{b}(z)\|\) close to \(\|z-z^*\|\);</li> <li>Jacobian matrix of \(\widehat{b}(z)\) is close to \(\Phi_z\), i.e. \(\|J_b(z) - \Psi_z\| \leq C\sigma/r^\prime + o_p(1)\);</li> <li>Hessian matrix of \(\widehat{b}(z)\) is lower bounded.</li> </ul> <div style="text-align: center;"> <img src="/assets/img/mf-pics/bias-vec.png" alt="Bias Vector" style="max-width: 80%; height: auto;"/> </div> <p>Finally, the manifold estimator is given by</p> \[\widehat{\mathcal{M}} = \{z\in\mathbb{R}^D: d(z,\mathcal{M})&lt;cr^\prime, \widehat{b}(z) = 0\}.\] <p>Under all the error bounds and all the smoothness, for any \(z^\prime\in\widehat{\mathcal{M}}\), with high probability,</p> <ul> <li>\(z^\prime\) is close to \(\mathcal{M}\);</li> <li>in its neighborhood, \(\widehat{b}(z)\) is rank \(D-d\).</li> </ul> <p>Hence, with high probability, \(\widehat{\mathcal{M}}\) is a \(d\)-dimension manifold, close to \(\mathcal{M}\), and its reach can be bounded via the Hessian of \(\widehat{b}(z)\).</p>]]></content><author><name></name></author><category term="manifold-fitting"/><category term="manifold-fitting"/><category term="ideas"/><summary type="html"><![CDATA[why we can 'fit' a manifold?]]></summary></entry><entry><title type="html">What is manifold fitting?</title><link href="https://sujiaji.github.io/blog/2024/mf/" rel="alternate" type="text/html" title="What is manifold fitting?"/><published>2024-04-10T04:00:00+00:00</published><updated>2024-04-10T04:00:00+00:00</updated><id>https://sujiaji.github.io/blog/2024/mf</id><content type="html" xml:base="https://sujiaji.github.io/blog/2024/mf/"><![CDATA[<p>This is a quick introduction of manifold fitting. A <a href="/assets/pdf/Quick_Notes_on_Manifold_Fitting.pdf">PDF version</a> is also available. More details can be found in:</p> <ul> <li>Fefferman, C., Ivanov, S., Kurylev, Y., Lassas, M., &amp; Narayanan, H. (2018, July). <a href="https://proceedings.mlr.press/v75/fefferman18a.html">Fitting a putative manifold to noisy data</a>. <em>In Conference On Learning Theory</em> (pp. 688-720). PMLR.</li> <li>Yao, Z., &amp; Xia, Y. (2019). <a href="https://arxiv.org/abs/1909.10228">Manifold fitting under unbounded noise</a>. <em>arXiv preprint arXiv:1909.10228</em>.</li> <li>Yao, Z., Su, J., Li, B., &amp; Yau, S. T. (2023). <a href="https://arxiv.org/abs/2304.07680">Manifold fitting</a>. <em>arXiv preprint arXiv:2304.07680</em>.</li> </ul> <h3 id="overview">Overview</h3> <p>In recent years, there has been a growing interest in non-Euclidean statistical analysis, particularly in the pursuit of recovering a low-dimensional structure, referred to as a manifold, that underlies high-dimensional data. The successful recovery of this manifold is contingent on certain noise concentration conditions. Previous methods tackle this challenge by approximating the manifold based on tangent space estimations at each data sample. While these methods offer theoretical convergence guarantees, they assume either noise-free data or noise with bounded characteristics. In practical scenarios where unbounded noise is common, the tangent space estimations at noisy samples become inherently imprecise, potentially introducing inaccuracies when fitting the manifold.</p> <p>In response to this challenge, our research project introduces a novel approach. Instead of estimating tangent spaces at the original data samples, we directly estimate these spaces at projected points on the underlying manifold. This strategic shift aims to mitigate errors caused by unbounded noise, resulting in more accurate manifold fitting.</p> <p>Our research, encompassing our 2019 paper (yx) and subsequent work in 2023 (ysl), centers on non-Euclidean statistical analysis, specifically the recovery of low-dimensional manifolds from high-dimensional data. Unlike existing methods relying on tangent space estimations at data samples, we directly estimate these spaces at manifold-projected points, enhancing accuracy and addressing unbounded noise. Our initial paper (yx) introduced a practical algorithm for manifold fitting, and our 2023 work (ysl) refines the algorithm and establishes superior error bounds. These contributions significantly advance non-Euclidean statistical analysis.</p> <h3 id="model-setting">Model Setting</h3> <div style="text-align: center;"> <img src="/assets/img/mf-pics/model.png" alt="Model Setting" style="max-width: 60%; height: auto;"/> </div> <p>Let \(\mathcal{M}\) be a \(d\)-dimensional smooth latent manifold embedded in the ambient space \(\mathbb{R}^D\). In this problem, we focus on a random vector \(Y \in \mathbb{R}^D\) that can be expressed as</p> \[Y = X + \xi,\] <p>where \(X \in \mathbb{R}^D\) is an unobserved random vector following a distribution \(\omega\) supported on the latent manifold \(\mathcal{M}\), and \(\xi \sim \phi_\sigma\) represents the ambient-space observation noise, independent of \(X\), with a standard deviation \(\sigma\). The distribution of \(Y\) can be viewed as the convolution of \(\omega\) and \(\phi_\sigma\), whose density at point \(y\) can be expressed as</p> \[\nu(y) = \int_\mathcal{M} \phi_\sigma(y-x)\omega(x)d x.\] <p>Assume \(\mathcal{Y} = \{y_i\}_{i=1}^N \subset \mathbb{R}^D\) is the collection of observed data points, also in the form of</p> \[y_i = x_i + \xi_i, \quad \text{ for } i = 1,\cdots,N,\] <p>with \((y_i, x_i,\xi_i)\) being \(N\) independent and identical realizations of \((Y,X,\xi)\). Based on \(\mathcal{Y}\), we construct an estimator \(\widehat{\mathcal{M}}\) for \(\mathcal{M}\) and provide theoretical justification for it under the following main assumptions:</p> <ul> <li> <p>The latent manifold \(\mathcal{M}\) is a compact and twice-differentiable \(d\)-dimensional sub-manifold, embedded in the ambient space \(\mathbb{R}^D\). Its volume with respect to the \(d\)-dimensional Hausdorff measure is upper bounded by \(V\), and its reach<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup> is lower bounded by a fixed constant \(\tau\).</p> </li> <li> <p>The distribution \(\omega\) is a smooth distribution, with respect to the \(d\)-dimensional Hausdorff measure, on \(\mathcal{M}\).</p> </li> <li> <p>The noise distribution \(\phi_\sigma\) is a Gaussian distribution supported on \(\mathbb{R}^D\) with density function</p> </li> </ul> \[\phi_\sigma (\xi)= (\frac{1}{2\pi \sigma^2})^{\frac{D}{2}}\exp{(-\frac{\|\xi\|_2^2}{2\sigma^2})}.\] <ul> <li>The intrinsic dimension \(d\) and noise standard deviation \(\sigma\) are known.</li> </ul> <p>The manifold estimator \(\widehat{\mathcal{M}}\) is suppose to be</p> <ul> <li>\(d\)-dimensional smooth manifold with lower bounded reach;</li> <li>close to \(\mathcal{M}\).</li> </ul> <div style="text-align: center;"> <img src="/assets/img/mf-pics/estimator.png" alt="Manifold estimator" style="max-width: 60%; height: auto;"/> </div> <hr/> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1"> <p>The value of \(\textnormal{reach}(\mathcal{M})\) can be interpreted as a second-order differential quantity if \(\mathcal{M}\) is treated as a function. Namely, for any arc-length parameterized geodesic \(\gamma\) of \(\mathcal{M}\), \(\|\gamma^{\prime\prime}(t)\|_2\leq \textnormal{reach}(\mathcal{M})^{-1}\) for all \(t\). <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="manifold-fitting"/><category term="manifold-fitting"/><category term="ideas"/><summary type="html"><![CDATA[what are we talking about when we say 'fit' a manifold?]]></summary></entry></feed>